{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:24.725571Z",
     "start_time": "2025-12-07T04:10:23.772815Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:27.185091Z",
     "start_time": "2025-12-07T04:10:24.736516Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('./coeqwalpackage')\n",
    "\n",
    "from coeqwalpackage.metrics import (read_in_df, add_water_year_column, compute_storage_thresholds, compute_metrics_suite, normalize_index, percent_change_from_baseline)\n",
    "from coeqwalpackage import cqwlutils as cu\n",
    "from coeqwalpackage import plotting as pu"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:27.213642Z",
     "start_time": "2025-12-07T04:10:27.191042Z"
    }
   },
   "source": [
    "pu.enable_headless_mode()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:27.232530Z",
     "start_time": "2025-12-07T04:10:27.223266Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_hex"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Notebook Configuration"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:27.248366Z",
     "start_time": "2025-12-07T04:10:27.236351Z"
    }
   },
   "source": [
    "# Control file settings\n",
    "CTRL_FILE = 'CalSim3DataExtractionInitFile_v4.xlsx'\n",
    "CTRL_TAB = 'Init'\n",
    "\n",
    "BASELINE_ID = \"s0020\"\n",
    "HIGHLIGHT_SCENARIOS = [10, 11, 20, 21, 23, 24]\n",
    "\n",
    "DPI = 300\n",
    "\n",
    "S_MLRTN_LEVEL5_TAF = 524 # Millerton Lake flood pool\n",
    "S_MLRTN_LEVEL1_TAF = 135 # Millerton Lake dead pool\n",
    "S_MELON_LEVEL1_TAF = 80 # New Melones dead pool\n",
    "\n",
    "VARIABLES_STORAGE = [\"S_SHSTA_\", \"S_OROVL_\", \"S_TRNTY_\",  \"S_SLUIS_CVP_\", \"S_SLUIS_SWP_\", \"S_MLRTN_\", \"S_MELON_\"]\n",
    "\n",
    "VARIABLES_FLOODPOOL = [\"S_SHSTALEVEL5DV\", \"S_OROVLLEVEL5DV\", \"S_TRNTYLEVEL5DV\", \"S_SLUIS_CVPLEVEL5DV\", \"S_SLUIS_SWPLEVEL5DV\", \"S_MLRTN_LEVEL5DV\", \"S_MELONLEVEL4DV\"]\n",
    "\n",
    "VARIABLES_DEADPOOL = [\"S_SHSTALEVEL1DV\", \"S_OROVLLEVEL1DV\", \"S_TRNTYLEVEL1DV\", \"S_SLUIS_CVPLEVEL1DV\", \"S_SLUIS_SWPLEVEL1DV\", \"S_MLRTN_LEVEL1DV\", \"S_MELONLEVEL1DV\"]\n",
    "\n",
    "FLOW_ANALYSIS_SCENARIO = \"s0018\""
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read from control file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:27.577270Z",
     "start_time": "2025-12-07T04:10:27.253784Z"
    }
   },
   "source": "ScenarioListFile, ScenarioListTab, ScenarioListPath, DVDssNamesOutPath, SVDssNamesOutPath, ScenarioIndicesOutPath, DssDirsOutPath, VarListPath, VarListFile, VarListTab, VarOutPath, DataOutPath, ConvertDataOutPath, ExtractionSubPath, DemandDeliverySubPath, ModelSubPath, GroupDataDirPath, ScenarioDir, DVDssMin, DVDssMax, SVDssMin, SVDssMax, NameMin, NameMax, DirMin, DirMax, IndexMin, IndexMax, StartMin, StartMax, EndMin, EndMax, VarMin, VarMax, DemandFilePath, DemandFileName, DemandFileTab, DemMin, DemMax, InflowOutSubPath, InflowFilePath, InflowFileName, InflowFileTab, InflowMin, InflowMax = cu.read_init_file(CTRL_FILE, CTRL_TAB)",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:31.929865Z",
     "start_time": "2025-12-07T04:10:27.581470Z"
    }
   },
   "source": [
    "df, dss_names = read_in_df(ConvertDataOutPath,DVDssNamesOutPath)\n",
    "df = add_water_year_column(df)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:31.958900Z",
     "start_time": "2025-12-07T04:10:31.943779Z"
    }
   },
   "source": [
    "print(f\"Loaded data frame shape: {df.shape}\")\n",
    "print(f\"Total scenarios: {len(dss_names)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data frame shape: (1200, 13020)\n",
      "Total scenarios: 35\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:32.053758Z",
     "start_time": "2025-12-07T04:10:31.965781Z"
    }
   },
   "source": "scenario_names = cu.build_scenario_labels_from_listing(ScenarioListPath, ScenarioListFile, ScenarioListTab)\nscenario_label_lookup = {f\"s{sid:04d}\": label for sid, label in scenario_names.items()}\nhighlight_ids = []\n\nfor item in HIGHLIGHT_SCENARIOS:\n    if isinstance(item, str) and item.lower().startswith('s'):\n        highlight_ids.append(item.lower())\n    else:\n        highlight_ids.append(f\"s{int(item):04d}\")\n        \nhighlight_ids = [hid.lower() for hid in highlight_ids]\ncolor_pool = [to_hex(c) for c in (plt.cm.tab10.colors + plt.cm.Set2.colors + plt.cm.Dark2.colors)]\nhighlight_colors = []\ncolor_idx = 0\n\nfor hid in highlight_ids:\n    if hid == BASELINE_ID.lower():\n        highlight_colors.append('#000000')\n    else:\n        highlight_colors.append(color_pool[color_idx % len(color_pool)])\n        color_idx += 1\n        \nhighlight_labels = [scenario_label_lookup.get(hid.lower(), hid.upper()) for hid in highlight_ids]\nhighlight_config = list(zip(highlight_ids, highlight_colors, highlight_labels))\nbaseline_label_text = scenario_label_lookup.get(BASELINE_ID.lower(), BASELINE_ID)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrics paths and make directories if needed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:32.076704Z",
     "start_time": "2025-12-07T04:10:32.069315Z"
    }
   },
   "source": [
    "metrics_root = Path(GroupDataDirPath) / \"metrics_output\"\n",
    "metrics_root.mkdir(parents=True, exist_ok=True)\n",
    "plots_dir = metrics_root / \"parallel_plots\"\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:32.083813Z",
     "start_time": "2025-12-07T04:10:32.079349Z"
    }
   },
   "source": "write_out_dfs = []",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:33.347622Z",
     "start_time": "2025-12-07T04:10:32.086004Z"
    }
   },
   "source": [
    "thresholds, threshold_outputs = compute_storage_thresholds(df, dss_names, VARIABLES_STORAGE, VARIABLES_DEADPOOL, VARIABLES_FLOODPOOL, mlrtn_level5constant=S_MLRTN_LEVEL5_TAF, mlrtn_level1constant=S_MLRTN_LEVEL1_TAF, smelon_level1constant=S_MELON_LEVEL1_TAF)\n",
    "\n",
    "write_out_dfs.extend(threshold_outputs)\n",
    "print(f\"Computed {len(thresholds)} storage threshold metrics\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables:\n",
      "['S_SHSTA_', 'S_OROVL_', 'S_TRNTY_', 'S_SLUIS_CVP_', 'S_SLUIS_SWP_', 'S_MLRTN_', 'S_MELON_']\n",
      "Current variable: S_SHSTA_\n",
      "Current variable: S_OROVL_\n",
      "Current variable: S_TRNTY_\n",
      "Current variable: S_SLUIS_CVP_\n",
      "Current variable: S_SLUIS_SWP_\n",
      "Current variable: S_MLRTN_\n",
      "Current variable: S_MELON_\n",
      "Computed 28 storage threshold metrics\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:33.364196Z",
     "start_time": "2025-12-07T04:10:33.352892Z"
    }
   },
   "source": [
    "# Flow Requirements - parameterized via FLOW_ANALYSIS_SCENARIO in config\n",
    "# Set FLOW_ANALYSIS_SCENARIO = None to skip this analysis\n",
    "\n",
    "if FLOW_ANALYSIS_SCENARIO is not None:\n",
    "    _scen = FLOW_ANALYSIS_SCENARIO\n",
    "    actual_flows = [f\"C_AMR004_{_scen}\", f\"C_TRN111_{_scen}\", f\"C_FTR029_{_scen}\", f\"C_MOK028_{_scen}\", f\"C_MCD005_{_scen}\", f\"C_SAC257_{_scen}\", f\"C_SJR127_{_scen}\", f\"C_SAC289_{_scen}\", f\"C_STS011_{_scen}\", f\"C_TUO003_{_scen}\", f\"C_YUB002_{_scen}\", f\"SP_SAC159_BTC003_{_scen}\", f\"C_SAC148_{_scen}\", f\"C_SAC122_{_scen}\" f\"C_FTR003_{_scen}\", f\"C_SAC049_{_scen}\", f\"C_SJR070_{_scen}\"]\n",
    "    \n",
    "    min_flows = [f\"C_AMR004_MIF_{_scen}\", f\"C_TRN111_MIF_{_scen}\", f\"C_FTR029_MIF_{_scen}\", f\"C_MOK028_MIF_{_scen}\", f\"C_MCD005_MIF_{_scen}\", f\"C_SAC257_MIF_{_scen}\", f\"C_SJR127_MIF_{_scen}\", f\"C_SAC289_MIF_{_scen}\", f\"C_STS011_MIF_{_scen}\", f\"C_TUO003_MIF_{_scen}\", f\"C_YUB002_MIF_{_scen}\", f\"SP_SAC159_BTC003_MIF_{_scen}\", f\"C_SAC148_MIF_{_scen}\", f\"C_SAC122_MIF_{_scen}\",f\"C_FTR003_MIF_{_scen}\", f\"C_SAC049_MIF_{_scen}\", f\"C_SJR070_MIF_{_scen}\"]\n",
    "    \n",
    "    print(f\"[INFO] Flow analysis enabled for scenario: {FLOW_ANALYSIS_SCENARIO}\")\n",
    "    \n",
    "else:\n",
    "    actual_flows = []\n",
    "    min_flows = []\n",
    "    print(\"[INFO] Flow analysis skipped (FLOW_ANALYSIS_SCENARIO is None)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Flow analysis enabled for scenario: s0018\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:10:33.371812Z",
     "start_time": "2025-12-07T04:10:33.367133Z"
    }
   },
   "source": [
    "# Metrics\n",
    "variables = [\"S_RESTOT_NOD_\", \"S_RESTOT_s\", \"S_SHSTA_\", \"S_OROVL_\", \"S_TRNTY_\", \"S_FOLSM_\", \"S_MELON_\", \"S_MLRTN_\",\"S_SLUIS_SWP\", \"S_SLUIS_CVP\", \"DEL_SWP_TOTAL_\", \"DEL_SWP_PMI_\", \"DEL_SWP_PAG_\", \"DEL_CVP_TOTAL_\", \"DEL_CVP_PAG_TOTAL_\", \"DEL_CVP_PSCEX_TOTAL_\", \"DEL_CVP_PRF_TOTAL_\", \"D_TOTAL_\", \"NDO_\", \"X2_PRV_KM_\",\"EM_EC_MONTH_\",\"JP_EC_MONTH\", \"DEL_CVP_PSC_N_\", \"DEL_CVP_PAG_N_\", \"DEL_CVP_PAG_S_\", \"DEL_CVP_PRF_S_\", \"DEL_CVP_PMI_N_\", \"DEL_CVP_PMI_S_\"] "
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.354818Z",
     "start_time": "2025-12-07T04:10:33.378154Z"
    }
   },
   "source": [
    "metrics, metric_outputs = compute_metrics_suite(df, dss_names, variables)\n",
    "write_out_dfs.extend(metric_outputs)\n",
    "print(f\"Computed {len(metrics)} metric categories: {list(metrics.keys())}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 59 metric categories: ['Apr_S_RESTOT_NOD_mnth_avg', 'Sept_S_RESTOT_NOD_mnth_avg', 'S_RESTOT_NOD_ann_avg', 'Apr_S_RESTOT_smnth_avg', 'Sept_S_RESTOT_smnth_avg', 'S_RESTOT_sann_avg', 'Apr_S_SHSTA__mnth_avg', 'Sept_S_SHSTA__mnth_avg', 'AprS_SHSTA__CV', 'Sept_S_SHSTA__CV', 'Apr_S_OROVL__mnth_avg', 'Sept_S_OROVL__mnth_avg', 'AprS_OROVL__CV', 'Sept_S_OROVL__CV', 'Apr_S_TRNTY__mnth_avg', 'Sept_S_TRNTY__mnth_avg', 'AprS_TRNTY__CV', 'Sept_S_TRNTY__CV', 'Apr_S_FOLSM__mnth_avg', 'Sept_S_FOLSM__mnth_avg', 'AprS_FOLSM__CV', 'Sept_S_FOLSM__CV', 'Apr_S_MELON__mnth_avg', 'Sept_S_MELON__mnth_avg', 'AprS_MELON__CV', 'Sept_S_MELON__CV', 'Apr_S_MLRTN__mnth_avg', 'Sept_S_MLRTN__mnth_avg', 'AprS_MLRTN__CV', 'Sept_S_MLRTN__CV', 'Apr_S_SLUIS_SWP_mnth_avg', 'Sept_S_SLUIS_SWP_mnth_avg', 'AprS_SLUIS_SWP_CV', 'Sept_S_SLUIS_SWP_CV', 'Apr_S_SLUIS_CVP_mnth_avg', 'Sept_S_SLUIS_CVP_mnth_avg', 'AprS_SLUIS_CVP_CV', 'Sept_S_SLUIS_CVP_CV', 'DEL_SWP_TOTAL__ann_avg', 'DEL_SWP_PMI__ann_avg', 'DEL_SWP_PAG__ann_avg', 'DEL_CVP_TOTAL__ann_avg', 'DEL_CVP_PAG_TOTAL__ann_avg', 'DEL_CVP_PSCEX_TOTAL__ann_avg', 'DEL_CVP_PRF_TOTAL__ann_avg', 'D_TOTAL__ann_avg', 'NDO__ann_avg', 'Fall_X2_PRV_KM__ann_avg', 'Spring_X2_PRV_KM__ann_avg', 'Fall_X2_PRV_KM__CV', 'Spring_X2_PRV_KM__CV', 'Fall_EM_EC_MONTH__ann_avg', 'Spring_EM_EC_MONTH__ann_avg', 'Fall_EM_EC_MONTH__CV', 'Spring_EM_EC_MONTH__CV', 'Fall_JP_EC_MONTH_ann_avg', 'Spring_JP_EC_MONTH_ann_avg', 'Fall_JP_EC_MONTH_CV', 'Spring_JP_EC_MONTH_CV']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out dataframes to single CSV"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.378771Z",
     "start_time": "2025-12-07T04:11:02.359532Z"
    }
   },
   "source": "write_out_dfs = [normalize_index(df) for df in write_out_dfs]\ncombined_df = pd.concat(write_out_dfs, axis=1)\ncombined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.396070Z",
     "start_time": "2025-12-07T04:11:02.381184Z"
    }
   },
   "source": [
    "combined_df.to_csv(metrics_root / \"all_metrics_output.csv\")"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the flow difference and plot it. (What was this for??)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.411968Z",
     "start_time": "2025-12-07T04:11:02.399669Z"
    }
   },
   "source": [
    "def calculate_flow_difference(actual_csv, mif_csv):\n",
    "    \"\"\"Calculate difference between actual and minimum instream flows from CSV exports.\"\"\"\n",
    "    actual = pd.read_csv(actual_csv, skiprows=6, names=[\"Index\", \"Date\", \"Flow\"])\n",
    "    actual[\"Date\"] = pd.to_datetime(actual[\"Date\"], format=\"%d%b%Y\", errors=\"coerce\")\n",
    "    actual[\"Flow\"] = pd.to_numeric(actual[\"Flow\"], errors=\"coerce\")\n",
    "    actual = actual.dropna(subset=[\"Date\", \"Flow\"])\n",
    "    actual.set_index(\"Date\", inplace=True)\n",
    "    actual.drop(columns=[\"Index\"], inplace=True)\n",
    "    actual.rename(columns={\"Flow\": \"ActualFlow\"}, inplace=True)\n",
    "\n",
    "    mif = pd.read_csv(mif_csv, skiprows=6, names=[\"Index\", \"Date\", \"Flow\"])\n",
    "    mif[\"Date\"] = pd.to_datetime(mif[\"Date\"], format=\"%d%b%Y\", errors=\"coerce\")\n",
    "    mif[\"Flow\"] = pd.to_numeric(mif[\"Flow\"], errors=\"coerce\")\n",
    "    mif = mif.dropna(subset=[\"Date\", \"Flow\"])\n",
    "    mif.set_index(\"Date\", inplace=True)\n",
    "    mif.drop(columns=[\"Index\"], inplace=True)\n",
    "    mif.rename(columns={\"Flow\": \"MinFlow\"}, inplace=True)\n",
    "\n",
    "    df = pd.concat([actual, mif], axis=1)\n",
    "    df[\"Difference\"] = df[\"ActualFlow\"] - df[\"MinFlow\"]\n",
    "    return df\n",
    "\n",
    "def plot_flow_difference(df, title=\"Min Flow Difference\", scenario_label=None):\n",
    "    if scenario_label is None:\n",
    "        scenario_label = FLOW_ANALYSIS_SCENARIO if FLOW_ANALYSIS_SCENARIO else \"scenario\"\n",
    "    \n",
    "    style = {\"color\": \"red\", \"linestyle\": \":\", \"label\": f\"{scenario_label} Adjusted Eflows\", \"linewidth\": 1.5}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(df.index, df[\"Difference\"], **style)\n",
    "    ax.axhline(0, linestyle=\"--\", color=\"gray\", linewidth=1)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel(\"Flow Difference (CFS)\", fontsize=12)\n",
    "    ax.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth=0.5)\n",
    "    ax.legend(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = title.replace(\" \", \"_\").replace(\":\", \"\").lower() + \".png\"\n",
    "    output_path = os.path.join(os.getcwd(), filename)\n",
    "    fig.savefig(output_path, dpi=DPI)\n",
    "    print(f\"Plot saved to: {output_path}\")\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallel Coordinates Plots"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.418568Z",
     "start_time": "2025-12-07T04:11:02.414470Z"
    }
   },
   "source": [
    "print(list(combined_df.columns))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All_Prob_S_SHSTA_flood', 'All_Prob_S_SHSTA_dead', 'Sept_Prob_S_SHSTA_flood', 'Sept_Prob_S_SHSTA_dead', 'All_Prob_S_OROVL_flood', 'All_Prob_S_OROVL_dead', 'Sept_Prob_S_OROVL_flood', 'Sept_Prob_S_OROVL_dead', 'All_Prob_S_TRNTY_flood', 'All_Prob_S_TRNTY_dead', 'Sept_Prob_S_TRNTY_flood', 'Sept_Prob_S_TRNTY_dead', 'All_Prob_S_SLUIS_CVP_flood', 'All_Prob_S_SLUIS_CVP_dead', 'Sept_Prob_S_SLUIS_CVP_flood', 'Sept_Prob_S_SLUIS_CVP_dead', 'All_Prob_S_SLUIS_SWP_flood', 'All_Prob_S_SLUIS_SWP_dead', 'Sept_Prob_S_SLUIS_SWP_flood', 'Sept_Prob_S_SLUIS_SWP_dead', 'All_Prob_S_MLRTN_flood', 'Sept_Prob_S_MLRTN_flood', 'All_Prob_S_MLRTN_dead', 'Sept_Prob_S_MLRTN_dead', 'All_Prob_S_MELON_flood', 'Sept_Prob_S_MELON_flood', 'All_Prob_S_MELON_dead', 'Sept_Prob_S_MELON_dead', 'Apr_Avg_S_RESTOT_NOD_TAF', 'Sep_Avg_S_RESTOT_NOD_TAF', 'Ann_Avg_S_RESTOT_NOD_TAF', 'Apr_Avg_S_RESTOT_sTAF', 'Sep_Avg_S_RESTOT_sTAF', 'Ann_Avg_S_RESTOT_sTAF', 'Apr_Avg_S_SHSTA_TAF', 'Sep_Avg_S_SHSTA_TAF', 'AprS_SHSTA__CV', 'SeptS_SHSTA__CV', 'Apr_Avg_S_OROVL_TAF', 'Sep_Avg_S_OROVL_TAF', 'AprS_OROVL__CV', 'SeptS_OROVL__CV', 'Apr_Avg_S_TRNTY_TAF', 'Sep_Avg_S_TRNTY_TAF', 'AprS_TRNTY__CV', 'SeptS_TRNTY__CV', 'Apr_Avg_S_FOLSM_TAF', 'Sep_Avg_S_FOLSM_TAF', 'AprS_FOLSM__CV', 'SeptS_FOLSM__CV', 'Apr_Avg_S_MELON_TAF', 'Sep_Avg_S_MELON_TAF', 'AprS_MELON__CV', 'SeptS_MELON__CV', 'Apr_Avg_S_MLRTN_TAF', 'Sep_Avg_S_MLRTN_TAF', 'AprS_MLRTN__CV', 'SeptS_MLRTN__CV', 'Apr_Avg_S_SLUIS_SWPTAF', 'Sep_Avg_S_SLUIS_SWPTAF', 'AprS_SLUIS_SWP_CV', 'SeptS_SLUIS_SWP_CV', 'Apr_Avg_S_SLUIS_CVPTAF', 'Sep_Avg_S_SLUIS_CVPTAF', 'AprS_SLUIS_CVP_CV', 'SeptS_SLUIS_CVP_CV', 'Ann_Avg_DEL_SWP_TOTAL_TAF', 'Ann_Avg_DEL_SWP_PMI_TAF', 'Ann_Avg_DEL_SWP_PAG_TAF', 'Ann_Avg_DEL_CVP_TOTAL_TAF', 'Ann_Avg_DEL_CVP_PAG_TOTAL_TAF', 'Ann_Avg_DEL_CVP_PSCEX_TOTAL_TAF', 'Ann_Avg_DEL_CVP_PRF_TOTAL_TAF', 'Ann_Avg_D_TOTAL_CFS', 'Ann_Avg_NDO_CFS', 'Fall_Ann_Avg_X2_PRV_KM_KM', 'Spring_Ann_Avg_X2_PRV_KM_KM', 'Fall_X2_PRV_KM__CV', 'Spring_X2_PRV_KM__CV', 'Fall_Ann_Avg_EM_EC_MONTH_UMHOS/CM', 'Spring_Ann_Avg_EM_EC_MONTH_UMHOS/CM', 'Fall_EM_EC_MONTH__CV', 'Spring_EM_EC_MONTH__CV', 'Fall_Ann_Avg_JP_EC_MONTHUMHOS/CM', 'Spring_Ann_Avg_JP_EC_MONTHUMHOS/CM', 'Fall_JP_EC_MONTH_CV', 'Spring_JP_EC_MONTH_CV']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.432079Z",
     "start_time": "2025-12-07T04:11:02.421435Z"
    }
   },
   "source": [
    "# Auto-generate metric groups from column patterns\n",
    "\n",
    "def build_metric_groups(df):\n",
    "    \"\"\"Generate metric groups by matching column name patterns.\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    \n",
    "    def match(patterns, exclude=None):\n",
    "        \"\"\"Return columns matching any pattern, optionally excluding others.\"\"\"\n",
    "        result = []\n",
    "        for c in cols:\n",
    "            c_str = str(c)\n",
    "            if any(p in c_str for p in patterns):\n",
    "                if exclude is None or not any(e in c_str for e in exclude):\n",
    "                    result.append(c)\n",
    "        return result\n",
    "    \n",
    "    groups = {\n",
    "        'G01_Floods': match(['Prob'], exclude=['dead']) + [c for c in cols if 'flood' in str(c).lower()],\n",
    "        'G02_Deadpools': match(['Prob'], exclude=['flood']) + [c for c in cols if 'dead' in str(c).lower()],\n",
    "        'G03_Storage_Apr': [c for c in cols if str(c).startswith('Apr') and ('S_' in str(c) or 'TAF' in str(c) or '_CV' in str(c))],\n",
    "        'G04_Storage_Sep': [c for c in cols if (str(c).startswith('Sep') or str(c).startswith('Ann_Avg_S_')) and ('S_' in str(c) or 'TAF' in str(c) or '_CV' in str(c))],\n",
    "        'G05_Deliveries_Flows': [c for c in cols if any(p in str(c) for p in ['DEL_', 'NDO_', 'D_TOTAL_'])],\n",
    "        'G06_Salinity': [c for c in cols if any(p in str(c) for p in ['X2_', 'EC_MONTH', 'EM_EC', 'JP_EC'])]}\n",
    "    \n",
    "    for k in groups:\n",
    "        groups[k] = list(dict.fromkeys(groups[k]))\n",
    "    \n",
    "    return groups\n",
    "\n",
    "GROUPS = build_metric_groups(combined_df)\n",
    "print(f\"Generated {len(GROUPS)} metric groups with {sum(len(v) for v in GROUPS.values())} total columns\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 6 metric groups with 101 total columns\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change from probability (0-1) to percent (0-100)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.445412Z",
     "start_time": "2025-12-07T04:11:02.434371Z"
    }
   },
   "source": [
    "prob_like_cols = [c for c in combined_df.columns if c.startswith(('All_Prob_', 'Sept_Prob_'))]\n",
    "if prob_like_cols:\n",
    "    to_scale = [c for c in prob_like_cols if np.nanmax(np.abs(combined_df[c].values)) <= 1.01]\n",
    "    print(f\"Scaling {len(to_scale)} / {len(prob_like_cols)} probability columns to percent.\")\n",
    "    combined_df[to_scale] = combined_df[to_scale] * 100.0\n",
    "else:\n",
    "    print(\"No probability-like columns found to scale.\")\n",
    "\n",
    "plots_dir = os.path.join(metrics_root, \"parallel_plots\")\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling 28 / 28 probability columns to percent.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.488641Z",
     "start_time": "2025-12-07T04:11:02.449027Z"
    }
   },
   "source": [
    "idx_lookup = {str(idx).strip().lower(): idx for idx in combined_df.index}\n",
    "baseline_key = BASELINE_ID.lower()\n",
    "if baseline_key in idx_lookup:\n",
    "    BASELINE_ROW = idx_lookup[baseline_key]\n",
    "else:\n",
    "    matches = [idx for idx in combined_df.index if baseline_key in str(idx).lower()]\n",
    "    if matches:\n",
    "        BASELINE_ROW = matches[0]\n",
    "    else:\n",
    "        raise KeyError(f\"Baseline '{BASELINE_ID}' not found in combined_df.index\")\n",
    "\n",
    "combined_df_pct = percent_change_from_baseline(combined_df, BASELINE_ROW)\n",
    "scenario_ids = list(map(str, combined_df.index))"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:02.502867Z",
     "start_time": "2025-12-07T04:11:02.493634Z"
    }
   },
   "source": [
    "idx_like = [str(i).strip().lower() for i in combined_df.index]\n",
    "def _resolve_label(hid):\n",
    "    if hid in idx_like:\n",
    "        return combined_df.index[idx_like.index(hid)]\n",
    "    hits = [combined_df.index[i] for i, lab in enumerate(idx_like) if hid in lab]\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "hl_idx = []\n",
    "hl_colors = []\n",
    "hl_labels = []\n",
    "missing = []\n",
    "\n",
    "for hid, color, label in highlight_config:\n",
    "    resolved = _resolve_label(hid)\n",
    "    if resolved is None:\n",
    "        missing.append(hid)\n",
    "        continue\n",
    "    if resolved in hl_idx:\n",
    "        continue\n",
    "    hl_idx.append(resolved)\n",
    "    hl_colors.append(color)\n",
    "    hl_labels.append(label)\n",
    "\n",
    "if missing:\n",
    "    print(f\"[WARN] highlight IDs not found in index and skipped: {missing}\")\n",
    "\n",
    "if BASELINE_ROW not in hl_idx:\n",
    "    hl_idx = [BASELINE_ROW] + hl_idx\n",
    "    hl_colors = ['#000000'] + hl_colors\n",
    "    hl_labels = [baseline_label_text] + hl_labels\n",
    "else:\n",
    "    order = hl_idx.index(BASELINE_ROW)\n",
    "    hl_idx = [hl_idx[order]] + hl_idx[:order] + hl_idx[order+1:]\n",
    "    hl_colors = ['#000000'] + hl_colors[:order] + hl_colors[order+1:]\n",
    "    hl_labels = [baseline_label_text] + hl_labels[:order] + hl_labels[order+1:]"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T04:11:20.069376Z",
     "start_time": "2025-12-07T04:11:02.506323Z"
    }
   },
   "source": [
    "for gname, gcols in GROUPS.items():\n",
    "    cols = cu.cols_present(combined_df, gcols)\n",
    "    if not cols:\n",
    "        print(f\"[SKIP] {gname}: no matching columns in combined_df.\")\n",
    "        continue\n",
    "\n",
    "    out_norm = os.path.join(plots_dir, f\"parallel_{gname}.png\")\n",
    "    pu.custom_parallel_coordinates_highlight_scenarios(objs=combined_df[cols], columns_axes=cols, axis_labels=cols, ideal_direction='top', minmaxs=['max'] * len(cols), color_dict_categorical={1: 'grey'}, highlight_indices=hl_idx, highlight_colors=hl_colors, highlight_descriptions=hl_labels, save_fig_filename=out_norm, title=f\"Parallel Line Plot — {gname}\", fontsize=10, figsize=(22, 8), dpi=DPI)\n",
    "\n",
    "    out_pct = os.path.join(plots_dir, f\"parallel_pct_{gname}.png\")\n",
    "    pu.custom_parallel_coordinates_highlight_scenarios_baseline_at_zero(objs=combined_df_pct[cols], columns_axes=cols, axis_labels=cols, highlight_indices=hl_idx, highlight_colors=hl_colors, highlight_descriptions=hl_labels, save_fig_filename=out_pct, title=f\"Change from Baseline ({BASELINE_ID}) — {gname}\", fontsize=10, figsize=(22, 8), dpi=DPI)\n",
    "\n",
    "    print(f\"Saved: {out_norm}\")\n",
    "    print(f\"Saved: {out_pct}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_G01_Floods.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_pct_G01_Floods.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_G02_Deadpools.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_pct_G02_Deadpools.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_G03_Storage_Apr.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_pct_G03_Storage_Apr.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_G04_Storage_Sep.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_pct_G04_Storage_Sep.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_G05_Deliveries_Flows.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_pct_G05_Deliveries_Flows.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_G06_Salinity.png\n",
      "Saved: ../../CalSim3_Model_Runs/Scenarios/Group_Data_Extraction/metrics_output/parallel_plots/parallel_pct_G06_Salinity.png\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}